# -*- coding: utf-8 -*-
"""Binding Affinity Predictor.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1_BvsKnuCg0B3oYg2CS1ucm9YviEf2h__
"""

#@title Install Dependencies
!pip -q install pandas numpy scikit-learn joblib

import io, json, joblib
import numpy as np, pandas as pd
from pathlib import Path
from google.colab import files

from sklearn.impute import SimpleImputer
from sklearn.pipeline import Pipeline
from sklearn.ensemble import RandomForestRegressor
from sklearn.feature_selection import VarianceThreshold

SEED = 42
np.random.seed(SEED)

#@title Run Prediction
# Install minimal deps
!pip -q install pandas numpy scikit-learn joblib openpyxl

import io, json, joblib
import numpy as np
import pandas as pd
from google.colab import files

print(" Upload your saved model bundle (.pkl) and a features file (.csv / .xlsx)â€¦")
uploaded = files.upload()
if not uploaded:
    raise ValueError("No files uploaded.")

paths = list(uploaded.keys())

# --- pick files ---
pkl_path = next((p for p in paths if p.lower().endswith(".pkl")), None)
data_path = next((p for p in paths if p.lower().endswith((".csv", ".xlsx", ".xls"))), None)
if not pkl_path or not data_path:
    raise ValueError("Please upload ONE .pkl model bundle and ONE .csv/.xlsx data file.")

# --- load model bundle (expects a dict with 'model' and 'features') ---
bundle = joblib.load(io.BytesIO(uploaded[pkl_path]))
if not isinstance(bundle, dict) or "model" not in bundle:
    raise ValueError("The .pkl should be a dict bundle with keys like {'model','features',...}.")

model      = bundle["model"]
feat_names = bundle.get("features")
if not feat_names:
    raise ValueError("Feature list missing in bundle under key 'features'.")

# --- read features table (in-memory) ---
if data_path.lower().endswith((".xlsx", ".xls")):
    df = pd.read_excel(io.BytesIO(uploaded[data_path]))
else:
    # try default csv; fallback to python engine if needed
    try:
        df = pd.read_csv(io.BytesIO(uploaded[data_path]))
    except Exception:
        df = pd.read_csv(io.BytesIO(uploaded[data_path]), engine="python")

# normalize column names
df.columns = [str(c).strip() for c in df.columns]

# try to find an ID column for convenience (PDB-like)
id_col = next((c for c in df.columns if "pdb" in c.lower()), None)
if id_col is None:
    id_col = next((c for c in df.columns if c.lower() in ("id","pdb_id","name","file","filename")), None)

# --- build X: select numeric features, ensure required columns exist ---
numeric = df.select_dtypes(include=[np.number]).copy()

# add any missing required features as NaN (so SimpleImputer in the pipeline can fill them)
for f in feat_names:
    if f not in numeric.columns:
        numeric[f] = np.nan

# enforce column order and drop extras
X = numeric[feat_names].astype(float)

# --- predict---
y_pred = model.predict(X)

# --- assemble output ---
out = pd.DataFrame({
    (id_col if id_col else "Index"): (df[id_col] if id_col else np.arange(len(df))),
    "Predicted_binding_affinity (kcal/mol)": y_pred
})

out_name = "Predictions_from_model.csv"
out.to_csv(out_name, index=False)
files.download(out_name)
print(f" Done! Saved {out_name} with {len(out)} rows.")